{
  "agent_id": "coder1",
  "task_id": "task_7",
  "files": [
    {
      "filename": "dynamic_difficulty_adjuster.py",
      "purpose": "Dynamic Difficulty Adjustment based on Flow Theory and eye tracking metrics",
      "priority": "high",
      "dependencies": [
        "numpy",
        "json"
      ],
      "key_functions": [
        "assess_user_state",
        "adjust_difficulty",
        "flow_state_detection"
      ],
      "estimated_lines": 180,
      "complexity": "high"
    }
  ],
  "project_info": {
    "project_name": "xr_eye_tracking_performance_framework",
    "project_type": "computer_vision",
    "description": "XR Space Framework for real-time eye tracking biofeedback to enhance human performance in VR/XR applications including training, screening, and teleoperation",
    "key_algorithms": [
      "velocity_threshold_identification",
      "saccade_detection",
      "fixation_detection",
      "dynamic_difficulty_adjustment",
      "real_time_biofeedback",
      "pupillometry_analysis"
    ],
    "main_libraries": [
      "unity3d",
      "sranipal_api",
      "opencv",
      "numpy",
      "pandas",
      "matplotlib",
      "scipy",
      "threading",
      "json",
      "math"
    ]
  },
  "paper_content": "PDF: cs.HC_2507.21000v1_Towards-Effective-Human-Performance-in-XR-Space-Fr.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nTowards Effective Human Performance\nin XR Space Framework based on\nReal-time Eye Tracking Biofeedback\nBarbara Karpowicz1,2[0000\u22120002\u22127478\u22127374], Tomasz\nKowalewski1,2[0009\u22120002\u22120542\u22129546], Pavlo Zinevych1,2[0009\u22120008\u22129250\u22128712],\nAdam Kuzdrali\u0144ski1[0000\u22120003\u22122383\u22121950], Grzegorz Marcin\nW\u00f3jcik3[0000\u22120003\u22122820\u2212397X], and Wies\u0142aw Kope\u01071,2[0000\u22120001\u22129132\u22124171]\n1XR Center, Polish-Japanese Academy of Information Technology\nkarpowicz.b@pja.edu.pl, kopec@pja.edu.pl\nhttps://www.xrc.pja.edu.pl\n2XR Space https://xrspace.xrlab.pl/\n3Maria Curie-Sk\u0142odowska University in Lublin\nAbstract. This paper proposes an eye tracking module for the XR\nSpace Framework aimed at enhancing human performance in XR-based\napplications, specifically in training, screening, and teleoperation. This\nframework provides a methodology and components that streamline the\ndevelopment of adaptive real-time virtual immersive systems. It contains\nmultimodal measurements - declarative in the form of in-VR question-\nnaires and objective, including eye tracking, body movement, and psy-\nchophysiological data (e.g., ECG, GSR, PPG). A key focus of this paper\nis the integration of real-time eye tracking data into XR environments\nto facilitate a biofeedback loop, providing insight into user attention,\ncognitive load, and engagement. Given the relatively high measurement\nfrequency of eye tracking - recognized as a noninvasive yet robust psy-\nchophysiological measure - this technology is particularly well suited for\nreal-time adjustments in task difficulty and feedback to enhance learn-\ning and operational effectiveness. Despite its established role in cognitive\nand attentional studies, implementing eye tracking metrics within dy-\nnamic, real-time XR environments poses unique challenges, particularly\ngiven the complex moving visuals presented in head-mounted displays\n(HMDs). This paper addresses these challenges by focusing on the es-\nsential aspects of integrating eye tracking in immersive systems based\non real-time engines, ultimately facilitating more efficient, adaptive XR\napplications.\nKeywords: Immersive Systems \u00b7Virtual Reality \u00b7eXtended Reality \u00b7\nHuman Performance \u00b7Psychophysiology \u00b7Eye tracking\n1 Introduction\nVirtual Reality (VR) and eXtended Reality (XR) technologies are transforming\nthe fields of training, screening, and teleoperation by enabling the creation ofarXiv:2507.21000v1  [cs.HC]  28 Jul 2025\n\n--- Page 2 ---\n2 B. Karpowicz et al.\nimmersive virtual environments that preserve ecological validity. When VR or\nXR technology is used, a high degree of immersion can be achieved, making par-\nticipants feel as if they are in a real environment. This increases the naturalness\nof their reactions and behavior. [6,13] The aspect of preserving ecological valid-\nity conditions in training or screening is important as it can affect the realism\nof user responses, the transferability of learned skills to real-world situations,\nand overall engagement and effectiveness of the XR experience. XR-based sys-\ntems enable realistic scenarios and interactions that may be difficult, expensive,\ndangerous, ethically controversial or even impossible to achieve in real life sit-\nuations. [8] XR Space Framework is intended for precise control over training\nvariables and conditions, allowing for consistent and repeatable scenarios that\nenhance skill acquisition and assessment accuracy. This approach combined with\nmultimodal measurements - declarative in the form of questionnaires in virtual\nreality, without the need to remove the VR goggles, and objective, including\nbody movement, eye tracking to measure executive/cognitive function, and psy-\nchophysiologicaldata(e.g.,ECG,GSR,PPG)-allowsforcomprehensiveinsights\ninto user states without disrupting immersion. In particular, eye tracking oper-\nates at relatively high frequencies, enabling Dynamic Difficulty Adjustment\n(DDA), feedback, and support based on the user\u2019s cognitive load. By contin-\nuously monitoring user responses, the XR Space Framework makes real-time\nadjustments possible, creating a more responsive and effective environment for\nlearning and operational tasks.\nThe maingoal ofthispaper isto present anintegrationof real-time eyetrack-\ning data with virtual immersive systems to enhance human performance .\nEye tracking technology provides invaluable insights into user attention, cogni-\ntive load, and engagement by monitoring gaze patterns (based on saccades ),\nfixations ,andpupilresponses .Byincorporatingthesemetrics,theframework\nenables the design of systems that dynamically adjust task difficulty, feedback,\nand learning pathways based on individual user performance in real-time.\n2 Related Works\nBased on our previous research on effective immersive systems for multimodal\ndata acquisition, we have successfully implemented real-time acquisition of eye\ntracking, body movement, and psychophysiological data across various XR-\ncontinuumimmersiveapplications.Thesesystemshavebeenappliedinnumerous\npsychology research studies, yielding valuable insights into user attention, cog-\nnitive load, and engagement in immersive environments.[5,7,10,11,13] While the\ndata acquisition phase is well-established, current efforts focus on developing\npre-processing and processing pipelines that enable real-time analysis and adap-\ntive responses. This includes dynamically adjusting task difficulty and feedback\nbased on the user\u2019s cognitive and physiological state, which remains a critical\nstep in enhancing the effectiveness and personalization of XR-based training and\nscreening systems.\n\n--- Page 3 ---\nTowards XR Framework for developing adaptive immersive systems 3\n2.1 Data Aquisition\nThe data acquisition process starts with calibration . In this state, the user is\nguided through a procedure to ensure accurate eye tracking performance. This\ntypically involves adjusting the position of HMD and IPD (interpupillary dis-\ntance), and lastly, the user looking at specific points in the virtual environment,\nallowing the system to align gaze data effectively. In our previous systems, 5-\npoint calibration was used. [5,7,10,11,13]\nReal-time acquisition of eye tracking data occurs continuously during the\nimmersive experience. The system logs key metrics, such as gaze direction, pupil\nsize, and blink status, using the integrated eye tracking SDK.\nTo manage the incoming data efficiently, a buffering system is imple-\nmented. This temporary storage holds eye tracking data points for a brief period,\nensuring a smooth saving process and preventing data loss during high-frequency\nsampling. The collected data are saved to a file in a separate thread. This logged\ndata can be used for a later analysis, enabling deeper insights into user behavior\nand cognitive load.\n2.2 Dynamic Difficulty Adjustment\nResearch on the topic of human motivation, performance, and emotions, based\non the Flow Theory [1] has been already integrated with the field of Human-\nComputer Interaction (HCI), and led to the emergence of the study area named\nAffective Computing (AC) [9,12].\nCompetence of the userDifficulty of the taskAnxiety\n(Too hard)\nBoredom\n(Too easy)EngagementFlow channel\nChange in task's difficulty\nChange in user's competence\nAutomatic adaptation of the\ntask difficulty\nFig. 1.Flow chart with suggested automatic adjustment of task difficulty (source: own\nelaboration, based on [1])\nWhen performing a task, the change between two emotional states can occur\ndue to one of the following reasons[2]:\n\u2013User\u2019s competence increases, but task difficulty stays the same - can cause\nboredom (green solid arrow in Figure 1)\n\n--- Page 4 ---\n4 B. Karpowicz et al.\n\u2013Task\u2019s difficulty increases too fast in comparison to user\u2019s competence in-\ncrease - can cause anxiety (red solid arrow in Figure 1)\nThe challenge lies in adapting task difficulty based on human performance.\nOneapproachisimplementingDynamicDifficultyAdjustmentusingeyetracking\ndata - specifically saccades, fixations, and pupil responses. This paper aims to\nprovide a proposition solution for the pre-processing and processing of eye\ntracking data in a real-time graphic engine , which can subsequently be\napplied to DDA.\nTable 1. Examples of eye tracking metrics for assessing engagement and emotional\nstate\nMetric What it measuresImportance in engagement\n/emotional state\nFixation\ndurationLength of focus on a point Long fixations indicate high engagement,\nshort fixations suggest scanning or disinter-\nest.\nSaccade\nvelocitySpeed of gaze movement\nbetween pointsHighvelocityindicatesexcitementorstress,\nlow velocity may suggest calm focus.\nPupil\ndilation\n/constrictionChanges in pupil size Larger pupils indicate higher cognitive load\nor emotional arousal. Small pupils may in-\ndicate relaxation, boredom, or disengage-\nment.\nBlink rate Frequency of blinks Frequent blinking can signal disengage-\nment, while fewer blinks suggest intense fo-\ncus.\nTime to first\nfixation\n(TTFF)Time to focus on a key el-\nement after stimulusShort TTFF indicates recognition or inter-\nest. Long TTFF suggests confusion.\nDwell time Total time spent looking\nat a specific object or areaLong dwell time on relevant items indicates\nengagement. Short dwell time shows dis-\ntraction.\n3 Universal Solution Proposal\nBased on our previous research and the above mentioned research context, as\nwell as state-of-the-art solutions, including HMDs with ET and contemporary\nRTCGengines,weproposethefollowinguniversalapproachtowarduniversalim-\nplementation of DDA, based on specific ET metrics and theoretical background.\n\n--- Page 5 ---\nTowards XR Framework for developing adaptive immersive systems 5\n3.1 Eye Tracking Metrics\nEye tracking metrics offer valuable insights into user engagement, involve-\nment, and emotional state by capturing the unconscious and subtle movements\nof the eyes.\nIn table 1 there are examples of eye tracking metrics, implemented in the\nproposed module, that can be used to assess engagement and emotional state.\nBy monitoring gaze patterns and pupil responses, it is possible to provide\nadaptive, more personalized feedback and instructions during training or screen-\ning sessions. Additionally, it gives a possibility to evaluate human perfor-\nmance. If the user\u2019s gaze aligns well with tasks objectives (e.g., looking at key\nobjects or following expected patterns), the level of difficulty can be increased.\nIf the system detects missed fixations or gaze moving too quickly over a cru-\ncial area, it gives information that this person needs more guidance and further\ntraining first, before increasing the level of difficulty.\n3.2 Theoretical Background\nVelocity-Threshold Identification is one of the most popular algorithms\nfor detecting saccades and fixations based on eye tracking data. It is based on\nmeasuring the angular velocity of eye movement and comparing it with a set\nthreshold to determine when there is fixation and when there is saccade. [3,4]\nEye movement angular velocity is calculated from the difference of gaze\npositions over successive samples time (see Equation 1).\nv=\u03b8\n\u2206t(1)\nwhere:\n\u2013\u03b8- angle between gaze direction vectors at moment of t1andt2\n\u2013\u2206t- time difference between t1andt2\nEquation 2 presents the calculation of an angle ( \u03b8)between the gaze direc-\ntion vectors in successive samples time.\n\u03b8=arccos (P1\u00b7P2\n|P1||P2|) (2)\nwhere:\n\u2013P1- gaze direction vector at the moment of t1\n\u2013P2- gaze direction vector at the moment of t2\n\u2013P1\u00b7P2- scalar product of P1andP2\n\u2013|P1||P2|- length of P1andP2\nDue to the fact that vectors P1andP2are normalized4, the denominator of\nthis equation can be omitted, resulting in the following equation 3:\n4Normalize vector has a length 1\n\n--- Page 6 ---\n6 B. Karpowicz et al.\n\u03b8=arccos (P1\u00b7P2) (3)\nThe decision whether a given sample is part of a saccade of fixation is based\non a given threshold.\nv > v threshold 1\u2192saccade\nv < v threshold 2\u2192fixation\nTypically saccades are roughly bounded by 250 deg/s and fixations can pot-\ntentially be identified with angular velocity less than 3 deg/s. [3]\n3.3 Universal Approach\nThe proposed solution utilizes the SRanipal API (associated with the HTC Vive\nPro Eye headset). However, the module was created with a universal approach.\nThe core advantage of this module lies in its flexibility and extensibility. It can\nintegrate with a range of eye tracking hardware and software platforms, such\nas Varjo5, Unity OpenXR6, and others. When it comes to Unity OpenXR, its\neye tracking input data is limited only to eye position and rotation (lacking\ndata needed for pupillometry). Therefore, it can be used only for saccades and\nfixations, without pupil responses.\nThe universal approach is achieved by implementing abstract classes and\ninterfaces, which act as standard templates for processing eye tracking data.\nDevelopers can easily extend these abstract classes to accommodate new data\nsources, ensuring that the module can adapt to future technologies or different\nplatforms without significant rework.\n3.4 Data Workflow\nWorkflow of eye tracking data in our proposition is presented in Figure 2. As\ndescribedinsubsection2.1,firstthereiseyetracker calibration .Afterthisstage\ndata acquisition is continuous during the XR experience and raw eye tracking\ndata is saved to the file for later analysis.\nData preprocessing focuses on cleaning and refining the raw eye tracking\ndata collected during the XR experience. This process starts with the removal of\ninvalid samples. Next, for each sample, the angular velocity is calculated. And\nlastly, there is a process of removing one-sample spikes using Median Filtering7.\nData processing involves analyzing the pre-processed eye tracking data to\nextract meaningful insights into user behavior and cognitive state. This includes\n5https://developer.varjo.com/docs/native/varjo-native-sdk\n6https://docs.unity3d.com/Packages/com.unity.xr.openxr@1.13/manual/features/\neyegazeinteraction.html\n7Median Filtering eliminates outliers by replacing each data point with the median of\nits neighboring points, which helps smooth out sudden spikes while preserving the\noverall trend of the data.\n\n--- Page 7 ---\nTowards XR Framework for developing adaptive immersive systems 7\nEye Tracker\nCalibrationData AcquisitionSaving raw data\nto the file\nData Preprocessing Data ProcessingDynamic Dif ficulty\nAdjustment\nFig. 2.Diagram presenting eye tracking data workflow (source: own elaboration)\ndetecting saccades andfixations , that are identified using velocity-threshold\nidentification, where rapid shifts in gaze direction exceeding a defined speed\nindicate a saccade, while fixations are determined by prolonged gaze on a specific\npoint. Additionally, changes in pupil diameter are monitored, serving as an\nindicator of cognitive load and emotional engagement. Together, these metrics\nprovide valuable information on user attention and mental effort that can be\nusedforDynamicDifficultyAdjustment inthefutureadaptiveenvironments\nmade for training, screening, and teleoperation.\n4 Discussion and Further Work\nHaving in mind the proposed universal approach towards the eye tracking mod-\nule, i.e. scalability, reliability, and two-way communication for real-time biofeed-\nback, there are several challenges. One of the most important areas is effi-\nciency, where the challenges relies on the inherent architecture of modern Real-\nTime Computer Graphics (RTCG) Systems. In particular, the presented module\nshould employ separate threads for data acquisition, preprocessing, and process-\ning to ensure smooth and real-time handling of eye tracking data. This archi-\ntecture allows for efficient performance, with each stage running independently.\nFor example, Unity\u2019s main thread is closely tied to frame rate (FPS), as it han-\ndles rendering, physics, and the core immersive system logic. Overloading the\nmain thread with tasks like data acquisition and processing can lead to frame\ndrops and reduced performance. By offloading these tasks to separate threads,\nthe module will ensure that the main thread remains focused on maintaining\nsmooth flow and high FPS, enhancing overall user experience.\nAcknowledgments\nThe authors did not receive any specific grant from funding agencies in the\npublic, commercial, or not-for-profit sectors for this article.\nThe authors have no competing interests to declare that are relevant to the\ncontent of this article.\n\n--- Page 8 ---\n8 B. Karpowicz et al.\nReferences\n1. Beck, L.A.: Csikszentmihalyi, mihaly.(1990). flow: the psychology of optimal expe-\nrience (1992)\n2. Chanel, G., Rebetez, C., B\u00e9trancourt, M., Pun, T.: Emotion assessment from phys-\niological signals for adaptation of game difficulty. Systems, Man and Cybernetics,\nPart A: Systems and Humans, IEEE Transactions on 41, 1052 \u2013 1063 (12 2011).\nhttps://doi.org/10.1109/TSMCA.2011.2116000\n3. Duchowski,A.T.,Krejtz,K.,Volonte,M.,Hughes,C.J.,Brescia-Zapata,M.,Orero,\nP.: 3d gaze in virtual reality: Vergence, calibration, event detection. Procedia Com-\nputer Science 207, 1641\u20131648 (2022). https://doi.org/https://doi.org/10.1016/j.\nprocs.2022.09.221, knowledge-Based and Intelligent Information and Engineering\nSystems: Proceedings of the 26th International Conference KES2022\n4. Imaoka, Y., Flury, A., de Bruin, E.D.: Assessing saccadic eye movements with\nhead-mounted display virtual reality technology. Frontiers in Psychiatry 11(2020).\nhttps://doi.org/10.3389/fpsyt.2020.572938\n5. Karpowicz, B., Mas\u0142yk, R., Skorupska, K., Jab\u0142o\u0144ski, D., Kalinowski, K.,\nKobyli\u0144ski,P.,Pochwatko,G.,Kornacka,M.,Kope\u0107,W.:Intergenerationalinterac-\ntionwithavatarsinvr:Anexploratorystudytowardsanxrresearchframework.In:\nDigital Interaction and Machine Intelligence. pp. 229\u2013238. Springer International\nPublishing, Cham (2022)\n6. Kisker, J., Gruber, T., Sch\u00f6ne, B.: Behavioral realism and lifelike psychophysiolog-\nical responses in virtual reality by the example of a height exposure. Psychological\nResearch 85(02 2021). https://doi.org/10.1007/s00426-019-01244-9\n7. Kope\u0107,W.,Pochwatko,G.,Kornacka,M.,Stawski,W.,Grzeszczuk,M.,Skorupska,\nK., Karpowicz, B., Mas\u0142yk, R., Zinevych, P., Knapi\u0144ski, S., et al.: Human factors\nin space exploration: Opportunities for international and interdisciplinary collab-\noration. In: Machine Intelligence and Digital Interaction Conference. pp. 339\u2013350.\nSpringer (2023)\n8. McIntosh, V.: Dialing up the danger: Virtual reality for the simulation of risk.\nFrontiers in Virtual Reality 3, 909984 (2022)\n9. Picard, R.W.: Affective Computing. MIT Press, Cambridge (1997)\n10. Pochwatko, G., Kopec, W., J\u0119drzejewski, Z., Jaskulska, A., Skorupska, K.H., Kar-\npowicz, B., Mas\u0142yk, R., Barnes, S., Grzeszczuk, M., Lazarek, J., Swidrak, J.: The\ninvisible \u2013 experienced: Developing and verifying a vr application for understand-\ning air pollution perception and attitudes. In: 2023 IEEE International Symposium\non Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). pp. 531\u2013536 (2023).\nhttps://doi.org/10.1109/ISMAR-Adjunct60411.2023.00114\n11. Pochwatko,G.,Kopec,W.,Swidrak,J.,Jaskulska,A.,Skorupska,K.H.,Karpowicz,\nB., Mas\u0142yk, R., Grzeszczuk, M., Barnes, S., Borkiewicz, P., et al.: Well-being in\nisolation: Exploring artistic immersive virtual environments in a simulated lunar\nhabitat to alleviate asthenia symptoms. In: Proceedings of the IEEE International\nSymposium on Mixed and Augmented Reality ISMAR. pp. 185\u2013194. IEEE (2023).\nhttps://doi.org/10.1109/ISMAR59233.2023.00033\n12. Rodrigues, P., Fonseca, M., Lopes, P.: Physiological-based difficulty assessment\nfor virtual reality rehabilitation games. In: Proceedings of FDG 2023. FDG \u201923,\nAssociation for Computing Machinery (2023). https://doi.org/10.1145/3582437.\n3587187\n13. Schudy, A., Pochwatko, G., Kope\u0107, W., Karpowicz, B., Skorupska, K., Grzeszczuk,\nM., Okruszek, L.: A revised and extended paradigm for social and non-social\n\n--- Page 9 ---\nTowards XR Framework for developing adaptive immersive systems 9\nstress elicitation in psychological research - a feasibility study in virtual reality.\nIn: 2023 IEEE International Symposium on Mixed and Augmented Reality Ad-\njunct (ISMAR-Adjunct) (2023)",
  "project_dir": "artifacts/projects/xr_eye_tracking_performance_framework",
  "communication_dir": "artifacts/projects/xr_eye_tracking_performance_framework/.agent_comm",
  "assigned_at": "2025-07-29T23:13:51.959131",
  "status": "assigned"
}